{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Free Network\n",
    "\n",
    "by outerskyb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from torchvision.io import read_image\n",
    "import random\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:30<00:00,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5 0 7 4 8 2 3 6 1]\n",
      " [0 1 2 3 4 5 6 7 8]\n",
      " [1 2 0 5 3 4 7 8 6]\n",
      " [2 3 1 0 5 6 8 4 7]\n",
      " [3 4 5 6 7 8 0 1 2]\n",
      " [4 5 3 8 6 7 1 2 0]\n",
      " [6 7 8 1 0 3 2 5 4]\n",
      " [7 8 6 2 1 0 4 3 5]\n",
      " [8 6 4 7 2 1 5 0 3]\n",
      " [0 1 2 3 6 8 5 7 4]\n",
      " [1 0 3 6 2 4 7 8 5]\n",
      " [2 3 0 7 8 1 4 5 6]\n",
      " [3 2 4 1 5 6 8 0 7]\n",
      " [4 6 1 0 7 5 2 3 8]\n",
      " [5 7 6 8 4 2 0 1 3]\n",
      " [8 4 5 2 0 7 3 6 1]\n",
      " [6 8 7 5 3 0 1 4 2]\n",
      " [7 5 8 4 1 3 6 2 0]\n",
      " [0 1 3 2 4 6 7 5 8]\n",
      " [1 0 2 8 7 5 4 6 3]\n",
      " [2 3 1 6 0 7 8 4 5]\n",
      " [3 2 0 7 6 4 5 8 1]\n",
      " [4 6 5 1 8 0 3 7 2]\n",
      " [5 4 6 3 2 8 1 0 7]\n",
      " [6 7 8 4 5 1 2 3 0]\n",
      " [7 8 4 5 1 3 0 2 6]\n",
      " [8 5 7 0 3 2 6 1 4]\n",
      " [0 1 3 7 2 5 4 6 8]\n",
      " [1 0 4 2 3 7 8 5 6]\n",
      " [2 3 1 8 7 6 5 0 4]\n",
      " [3 4 7 0 6 1 2 8 5]\n",
      " [4 6 2 5 0 8 7 1 3]\n",
      " [8 7 0 6 5 4 1 3 2]\n",
      " [5 2 6 4 8 0 3 7 1]\n",
      " [6 5 8 1 4 3 0 2 7]\n",
      " [7 8 5 3 1 2 6 4 0]\n",
      " [0 1 2 4 6 3 8 7 5]\n",
      " [1 0 3 2 7 4 5 8 6]\n",
      " [2 3 1 0 5 8 7 6 4]\n",
      " [3 2 0 6 4 7 1 5 8]\n",
      " [4 5 6 8 0 1 3 2 7]\n",
      " [5 4 7 1 8 6 0 3 2]\n",
      " [6 7 8 3 1 5 2 4 0]\n",
      " [7 8 4 5 2 0 6 1 3]\n",
      " [8 6 5 7 3 2 4 0 1]\n",
      " [0 1 2 6 3 7 4 5 8]\n",
      " [1 0 3 5 2 6 7 8 4]\n",
      " [2 3 4 1 8 0 5 7 6]\n",
      " [3 2 0 4 7 8 1 6 5]\n",
      " [4 5 6 7 0 3 8 2 1]\n",
      " [5 4 7 8 6 1 0 3 2]\n",
      " [6 8 5 2 1 4 3 0 7]\n",
      " [8 7 1 0 5 2 6 4 3]\n",
      " [7 6 8 3 4 5 2 1 0]\n",
      " [0 1 4 5 8 6 7 3 2]\n",
      " [1 0 6 4 5 8 3 2 7]\n",
      " [2 4 1 8 0 3 5 7 6]\n",
      " [3 5 8 6 7 4 2 0 1]\n",
      " [8 7 2 0 6 5 4 1 3]\n",
      " [4 2 0 7 3 1 8 6 5]\n",
      " [5 3 7 2 1 0 6 8 4]\n",
      " [6 8 5 3 2 7 1 4 0]\n",
      " [7 6 3 1 4 2 0 5 8]\n",
      " [0 5 3 4 7 6 1 2 8]\n",
      " [2 0 4 8 5 1 7 6 3]\n",
      " [5 4 2 7 3 0 8 1 6]\n",
      " [1 6 8 2 4 5 3 0 7]\n",
      " [4 1 0 6 8 3 2 7 5]\n",
      " [3 7 6 0 2 8 4 5 1]\n",
      " [6 8 7 5 1 4 0 3 2]\n",
      " [7 3 5 1 0 2 6 8 4]\n",
      " [8 2 1 3 6 7 5 4 0]\n",
      " [0 5 6 4 8 7 1 3 2]\n",
      " [2 7 4 0 6 3 8 1 5]\n",
      " [1 0 2 5 3 8 4 6 7]\n",
      " [3 1 7 8 0 6 2 5 4]\n",
      " [4 2 3 1 7 0 5 8 6]\n",
      " [6 8 1 3 2 5 7 4 0]\n",
      " [5 6 0 7 1 4 3 2 8]\n",
      " [7 4 8 2 5 1 6 0 3]\n",
      " [8 3 5 6 4 2 0 7 1]\n",
      " [1 0 2 6 4 3 8 7 5]\n",
      " [0 1 5 7 8 6 3 2 4]\n",
      " [2 3 8 0 5 4 6 1 7]\n",
      " [3 2 0 8 7 1 4 5 6]\n",
      " [6 4 7 3 2 0 5 8 1]\n",
      " [5 6 3 2 1 7 0 4 8]\n",
      " [7 8 4 1 6 5 2 3 0]\n",
      " [4 7 6 5 3 8 1 0 2]\n",
      " [8 5 1 4 0 2 7 6 3]\n",
      " [0 1 6 4 7 3 8 2 5]\n",
      " [1 6 0 8 3 4 2 5 7]\n",
      " [2 7 1 0 5 8 4 6 3]\n",
      " [6 8 3 1 2 7 5 4 0]\n",
      " [3 5 7 2 4 6 0 8 1]\n",
      " [4 3 5 7 8 1 6 0 2]\n",
      " [5 0 4 6 1 2 3 7 8]\n",
      " [7 2 8 5 6 0 1 3 4]\n",
      " [8 4 2 3 0 5 7 1 6]\n",
      " [1 0 7 5 4 6 3 2 8]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "import numpy as np\n",
    "import itertools\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "classes = 100\n",
    "selection = 'max'\n",
    "\n",
    "\n",
    "P_hat = np.array(list(itertools.permutations(list(range(9)), 9)))\n",
    "n = P_hat.shape[0]\n",
    "\n",
    "for i in trange(classes):\n",
    "    if i==0:\n",
    "        j = np.random.randint(n)\n",
    "        P = np.array(P_hat[j]).reshape([1,-1])\n",
    "    else:\n",
    "        P = np.concatenate([P,P_hat[j].reshape([1,-1])],axis=0)\n",
    "    \n",
    "    P_hat = np.delete(P_hat,j,axis=0)\n",
    "    D = cdist(P,P_hat, metric='hamming').mean(axis=0).flatten()\n",
    "    \n",
    "    if selection=='max':\n",
    "        j = D.argmax()\n",
    "    else:\n",
    "        m = int(D.shape[0]/2)\n",
    "        S = D.argsort()\n",
    "        j = S[np.random.randint(m-10,m+10)]\n",
    "    \n",
    "        \n",
    "print(P)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cuda check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "IMAGE_DIR = './VOCdevkit/VOC2012/JPEGImages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Voc2012Dataset(Dataset):\n",
    "    def __init__(self,img_dir,permutation_set,transform = None ):\n",
    "        self.img_paths = os.listdir(img_dir)\n",
    "        self.transform = transform\n",
    "        self.permutation_set = permutation_set\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)*69\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        img_path = os.path.join(IMAGE_DIR, self.img_paths[random.randint(0,len(self.img_paths)-1)])\n",
    "        mat = cv2.imread(img_path)\n",
    "        mat = cv2.resize(mat,(225,225))\n",
    "        images = list()\n",
    "        for i in range(3):\n",
    "            for j in range(3):\n",
    "                rx = random.randint(0,10)\n",
    "                ry = random.randint(0,10)\n",
    "                roi = mat[i*75+ry:i*75+ry+64,j*75+rx:j*75+rx+64,:]\n",
    "                images.append(torch.Tensor(cv2.resize(roi,(75,75))).permute(2,0,1).to(device))\n",
    "        \n",
    "        permutation = random.randint(0,len(self.permutation_set)-1)\n",
    "        \n",
    "        out = list()\n",
    "        for i in range(0,9):\n",
    "            out.append(images[self.permutation_set[permutation][i]].to(device))\n",
    "        return out, torch.Tensor([permutation]).to(device)#torch.nn.functional.one_hot(torch.Tensor(permutation),num_classes=len(self.permutation_set-1)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Voc2012Dataset(IMAGE_DIR,P)\n",
    "dataloader = DataLoader(dataset,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Defining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CFN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CFN,self).__init__()\n",
    "        \n",
    "        self.siames = nn.Sequential(\n",
    "            nn.Conv2d(3,96,(11,11),2,0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((3,3),2),\n",
    "            nn.LocalResponseNorm(5,0.0001,0.75,2),\n",
    "            nn.Conv2d(96,256,(5,5),1,2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((3,3),2),\n",
    "            nn.LocalResponseNorm(5,0.0001,0.75,2),\n",
    "            nn.Conv2d(256,384,(3,3),1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384,384,(3,3),1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384,256,(3,3),1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((3,3),2,0),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3*3*256,512)\n",
    "        )\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(4608,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4096,100),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = torch.Tensor().to(device)\n",
    "        for idx in range(0,9):\n",
    "            y = torch.cat((y,self.siames(x[idx])),1)\n",
    "        representation = self.flatten(y)\n",
    "        out = self.fc(representation)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = CFN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.2\n",
    "num_epoches = 100\n",
    "optimizier = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "size = len(dataloader.dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/100: : 0it [00:00, ?it/s]C:\\Program Files\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py:117: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n",
      "Epoch 0/100: : 496it [06:58,  1.23it/s, epoch: 0 loss: 4.614395 batch: [  490/ 9231] ]"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epoches):\n",
    "    pgbar = tqdm(enumerate(dataloader))\n",
    "    pgbar.set_description(f\"Epoch {epoch}/{num_epoches}\")\n",
    "    for batch_idx, (images,y) in pgbar:\n",
    "        \n",
    "        pred = model(images)\n",
    "        #y = nn.functional.one_hot(y,num_classes=100).to(device)\n",
    "        loss = loss_fn(pred,y.type(torch.LongTensor).to(device).squeeze())\n",
    "        \n",
    "        optimizier.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizier.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            loss, current = loss.item(), batch_idx \n",
    "            pgbar.set_postfix_str(f\"epoch: {epoch} loss: {loss:>7f} batch: [{batch_idx:>5d}/{size//128:>5d}] \")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4ba962433ce634cde499a2def155867a3065a4841db75053b162653b9c20e813"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
